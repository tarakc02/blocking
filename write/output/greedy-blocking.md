# Introduction

## A too-big problem

In our work, we often rely on multiple overlapping data sources in order
to make inferences in the face of the sorts of [incompleteness and
selection bias](https://hrdag.org/coreconcepts/) that characterize
[convenience
samples](https://hrdag.org/2013/04/05/convenience-samples-what-they-are/).
But that leaves us with the problem of [entity
resolution](https://en.wikipedia.org/wiki/Record_linkage) – of all the
records we have, from all of the sources we’ve included, which ones
correspond to the same distinct entity? Given a suitable similarity
metric over the records, we can answer this question by clustering the
records using the similarity metric (in which case, each cluster is an
entity).

Before we’ve even defined or selected the appropriate metric, we know
that it will be somewhat expensive to calculate. If we wanted to
calculate the full similarity matrix for all records in a database of
\(n\) records, that leaves us having to do \(n^2\) of these expensive
calculations, which quickly becomes too many.

Notice, though, that whatever similarity metric we use, and whatever
procedure we use to assign records to clusters, we know that records
that are not very similar should not end up in the same cluster. So we
can “round” the similarity down to 0 for sufficiently dissimilar
records, and we are left with the much smaller job of filling out the
cells of a sparse matrix, rather than the full \(n^2\) cells of a dense
matrix.

But that all presumes that we can, without actually calculating all of
the similarities, identify the pairs of records that are sufficiently
similar (so that we only calculate those similarities). That is the
problem I’ll address in this post – how to efficiently identify a small
subset of candidates among all \(n^2\) possible pairs of records that
still contains all (or most) of the pairs that do in fact refer to the
same entity?

This post builds on [this 5-part series by Patrick
Ball](https://hrdag.org/tech-notes/adaptive-blocking-writeup-1.html)

## Blocking

There are a number of functions over the records in our database that
map similar (but not necessarily identical) records to the same output
value. For instance, \(name(record)\) takes in a record and returns the
string contained in the name column of that record. \(date(record)\)
does the same for dates. \(soundex(name(record))\) returns the [Soundex
code](https://en.wikipedia.org/wiki/Soundex) associated with the name
column of a record, while \(year(date(record))\) returns the year
component of the date. We’ll call these functions *rules*, since they
correspond to a rule for generating candidate pairs: cut the data into
blocks of records that have the same return value for a given rule (such
as all records with the name “Gabriela Doe”), and only consider pairs
within these blocks. This technique is known as blocking. See
[here](https://hrdag.org/tech-notes/adaptive-blocking-writeup-2.html)
for more examples of blocking rules.

The sorts of rules described above, by themselves, will in general be
insufficient: for example, \(year(date(record))\) may still generate too
many pairs to be practical, while \(name(record)\) may miss pairs that
are in fact similar (imagine a record with the name “Gabby Doe”).
Instead we’ll produce numerous such rules and combine them into more
complex rules. We combine rules via the \(Conjunction\) operator, so
that for example \(name \wedge date\) consists of all blocks of records
that have the same name and the same date. We’ll attempt to find several
conjunctions, and output the union of the pairs generated by each one.
We call the operator that takes two rules and returns a new rule that
generates the union of the pairs from each constituent rule the
\(Disjunction\) operator. So our final rule could be like:

\[
(name \wedge date) \vee
(location \wedge soundex(name) \wedge year(date)) \vee
(...)
\]

Through large amounts of manual effort, we collect labeled training data
that tells us, for a given pair of records, if those records represent
the same underlying entity. Then our task is to find a disjunction of
conjunctions, as above, that identifies as many of the true pairs in the
training data as possible, subject to a constraint on how many total
pairs we’re willing to generate. Alas, if we have developed \(k\) simple
rules, there are \(2^k\) distinct conjunctions of these rules, and
\(2^{2^k}\) disjunctions of conjunctions\!

# Data

I simulated some training data that contains duplicates, and a number of
columns that can be used as simple rules:

``` julia
using Feather
records = Feather.read("input/small-recs.feather");
pairs = Feather.read("input/small-pairs.feather");
records
```

    ## 1612×13 DataFrame. Omitted printing of 5 columns
    ## │ Row  │ id    │ r1     │ r2     │ r3     │ da2    │ da3    │ db2    │ db3    │
    ## │      │ Int32 │ String │ String │ String │ String │ String │ String │ String │
    ## ├──────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┼────────┤
    ## │ 1    │ 1     │ abd    │ caaacd │ bbcbac │ caa    │ bbc    │ aaa    │ bcb    │
    ## │ 2    │ 2     │ ddc    │ cdcdba │ aabcac │ cdc    │ aab    │ dcd    │ abc    │
    ## │ 3    │ 3     │ bcb    │ dabbdc │ cbcccc │ dab    │ cbc    │ abb    │ bcc    │
    ## │ 4    │ 4     │ abb    │ bdcdaa │ cabcca │ bdc    │ cab    │ dcd    │ abc    │
    ## │ 5    │ 5     │ cdb    │ cabcaa │ baacca │ cab    │ baa    │ abc    │ aac    │
    ## │ 6    │ 6     │ bab    │ ccccba │ aacabb │ ccc    │ aac    │ ccc    │ aca    │
    ## │ 7    │ 7     │ cbc    │ bcacac │ babcca │ bca    │ bab    │ cac    │ abc    │
    ## ⋮
    ## │ 1605 │ 296   │ cbd    │ ddadcb │ ababbb │ bbb    │ acb    │ bbc    │ cba    │
    ## │ 1606 │ 384   │ bcd    │ cdcdaa │ bacaaa │ bbc    │ bac    │ bca    │ aca    │
    ## │ 1607 │ 877   │ bab    │ cadcdd │ acabcc │ cad    │ aca    │ adc    │ cab    │
    ## │ 1608 │ 714   │ bdd    │ accadb │ aacaab │ acc    │ caa    │ cca    │ abb    │
    ## │ 1609 │ 436   │ bac    │ acdadc │ aacabb │ acd    │ aac    │ cda    │ aca    │
    ## │ 1610 │ 419   │ bda    │ bacdab │ abbcac │ bac    │ abb    │ acd    │ bbc    │
    ## │ 1611 │ 528   │ bdc    │ cabdbc │ babbbb │ cdc    │ bab    │ dcd    │ abb    │
    ## │ 1612 │ 106   │ cdc    │ bdaaac │ baabcb │ bda    │ baa    │ daa    │ aab    │

`pairs` contains the record ids of each identified pair, along with
indicators for each blocking rule of whether that pair would be included
in the blocks defined by the rule:

``` julia
pairs
```

    ## 800×14 DataFrame. Omitted printing of 6 columns
    ## │ Row │ pairid │ recordid_1 │ recordid_2 │ r1   │ r2   │ r3   │ da2  │ da3  │
    ## │     │ Int32  │ Int32      │ Int32      │ Bool │ Bool │ Bool │ Bool │ Bool │
    ## ├─────┼────────┼────────────┼────────────┼──────┼──────┼──────┼──────┼──────┤
    ## │ 1   │ 1      │ 5          │ 1340       │ 1    │ 1    │ 1    │ 1    │ 1    │
    ## │ 2   │ 2      │ 5          │ 1566       │ 0    │ 1    │ 1    │ 1    │ 1    │
    ## │ 3   │ 3      │ 6          │ 1099       │ 1    │ 1    │ 1    │ 1    │ 1    │
    ## │ 4   │ 4      │ 7          │ 1069       │ 1    │ 1    │ 1    │ 1    │ 1    │
    ## │ 5   │ 5      │ 8          │ 1260       │ 1    │ 0    │ 0    │ 1    │ 1    │
    ## │ 6   │ 6      │ 9          │ 1247       │ 0    │ 1    │ 1    │ 1    │ 1    │
    ## │ 7   │ 7      │ 14         │ 1231       │ 1    │ 1    │ 0    │ 1    │ 1    │
    ## ⋮
    ## │ 793 │ 793    │ 1463       │ 1481       │ 1    │ 1    │ 1    │ 1    │ 1    │
    ## │ 794 │ 794    │ 1465       │ 1598       │ 0    │ 1    │ 0    │ 1    │ 1    │
    ## │ 795 │ 795    │ 1470       │ 1528       │ 1    │ 1    │ 1    │ 1    │ 1    │
    ## │ 796 │ 796    │ 1483       │ 1601       │ 0    │ 1    │ 1    │ 1    │ 1    │
    ## │ 797 │ 797    │ 1522       │ 1548       │ 1    │ 0    │ 0    │ 0    │ 1    │
    ## │ 798 │ 798    │ 1560       │ 1573       │ 0    │ 1    │ 1    │ 1    │ 1    │
    ## │ 799 │ 799    │ 1567       │ 1610       │ 1    │ 1    │ 1    │ 1    │ 1    │
    ## │ 800 │ 800    │ 1570       │ 1580       │ 0    │ 1    │ 0    │ 1    │ 1    │

# Solution framework

I call the total number of pairs generated by a rule the `cost` of the
rule, and the number of true matches included within that total the
`value` of the rule. I seek a combination of rules that maximizes total
`value`, subject to a fixed `budget`. The plan is to search for
conjunctions bit by bit – I search for a good conjunction, then remove
the true pairs that are covered by that conjunction from the set of
identified pairs in order to create a new problem, with the reduced set
of identified pairs and the reduced budget that results from subtracting
the cost of the conjunction from the original budget. In particular,
I’ll try to find conjunctions that have a high ratio of `value` to
`cost` at each iteration, and continue until there are no more pairs to
find, or I run out of `budget`.

I used [Julia](https://julialang.org/) to search for rules. The type
`Blocking.Problem` contains the relevant information related to a
problem:

``` julia
Base.MainInclude.include("../block/src/block.jl");
using .Blocking
problem = Blocking.Problem(
    records,
    pairs,
    # array of candidate rules
    [n for n in propertynames(records) if n ∉ (:id, :recordid)],
    # budget
    5000);
problem.npairs
```

    ## 800

In order to search for good conjunctions, I use a type of [neighborhood
search](https://en.wikipedia.org/wiki/Local_search_\(optimization\)).
The idea is to start with some proposed solution, and make small
modifications until I reach some stopping criterion. Each small
modification is called a *local move,* and the set of all available
local moves from a given proposed solution is called the *neighborhood*
of that solution. In this case, the *neighborhood* of a given
conjunction consists of all conjunctions that I can create by adding or
removing exactly one conjunct from the conjunction.

So at the highest level, I’m looking to build an array of conjunctions,
each one aimed at covering any true pairs that were missed by the
previous conjunctions:

``` julia
using .Blocking.Conjunctions
using .Blocking.Search

function search(problem)
    result = Blocking.Solution[]
    while problem.npairs > 0
        solution = Blocking.Search.greedy(
            problem,
            Blocking.Conjunctions.empty(problem),
            Blocking.Search.add_greedy)
        solution.cost > problem.budget && break
        solution.value == 0 && break
        append!(result, [solution])
        problem = Blocking.Conjunctions.subproblem(solution)
    end
    return result
end
```

    ## search (generic function with 1 method)

Within each iteration, I use `Blocking.Search.greedy` to find the next
conjunction. For the conjunction search, I begin with an empty
conjunction, and add conjuncts one at a time. At each step, I add the
conjunct that results in the highest `value/cost` ratio, and stop when I
can no longer improve the ratio. This logic is implemented in the module
`Blocking.Search`. Specifically, `greedy` is a function that takes a
blocking problem, a starting point, and a function that advances from
one conjunction to the next until there are no remaining local moves
that would lead to an improvement:

The code for `Blocking.search.greedy` is not too complicated –

``` julia
function greedy(problem, start, advance)
    current = start
    next = advance(current)
    while next != current
        current = next
        next = advance(current)
    end
    return next
end;
```

The `advance` function should take a conjunction and return a new
conjunction that is in the neighborhood of available local moves. It
should return the original conjunction if there are no local moves
available that are improvements on the current position. The one bit of
complication to `Blocking.Search.add_greedy` is that I want to avoid
returning a conjunction whose cost would exceed my budget. I do that by
continuing to add conjuncts until I have an affordable conjunction:

``` julia
function add_greedy(conjunction)
    problem = conjunction.problem
    current = conjunction
    current_ratio = current.value / current.cost
    for rule in conjunction.unselected
        candidate = Conjunctions.and(conjunction, rule)
        candidate_ratio = candidate.value / candidate.cost
        candidate_ratio < current_ratio && continue
        if candidate_ratio > current_ratio
            current = candidate
            current_ratio = candidate_ratio
            continue
        end

        if candidate.value > current.value || candidate.cost < current.cost
            current = candidate
            current_ratio = candidate_ratio
        end
    end
    # if we end up with a solution that is too expensive, just add more
    # conjuncts until we have something acceptable
    if current.cost > problem.budget && !isempty(current.unselected)
        current == conjunction && return current
        return add_greedy(current)
    end
    return current
end;
```

# Results

``` julia
result = search(problem);
[show(rule) for rule in result]
```

    ## 10-element Array{String,1}:
    ##  "r3 ∧ r1 ∧ r2"
    ##  "db3 ∧ dc2 ∧ r2 ∧ da2"
    ##  "r3 ∧ dd2 ∧ db2"
    ##  "dc3 ∧ da3 ∧ r2"
    ##  "r3 ∧ dc2 ∧ da3 ∧ da2"
    ##  "dd3 ∧ r2"
    ##  "dd3 ∧ r1 ∧ dc2 ∧ dd2"
    ##  "r3 ∧ db3 ∧ r1 ∧ dc3"
    ##  "db3 ∧ r1 ∧ dd2 ∧ r2"
    ##  "db3 ∧ dd2 ∧ dc3 ∧ da2"

``` julia
# number of true matches identified:
sum([rule.value for rule in result])
```

    ## 791

``` julia
# total number of pairs generated:
sum([rule.cost for rule in result])
```

    ## 4470

The neighborhood search strategy results in conjunctions of various
sizes, that together efficiently cover almost all of the true matches.
In order to emit the pairs:

``` julia
# emits pairs from a single conjunction
function emit(solution)
    recs = solution.problem.records
    cols = union(collect(solution.selected), [:recordid])
    blocks = filter(row -> row.recordid < row.recordid_1,
                    join(solution.problem.records[!, cols],
                         solution.problem.records[!, cols],
                         on = collect(solution.selected), makeunique = true))
    blocks[!, [:recordid, :recordid_1]]
end;

# takes the union of all pairs emitted by each conjunction
# [a; b] binds the rows of data frames a and b
function generate_pairs(result)
    reduce((a,b) -> [a;b], [emit(solution) for solution in result])
end;
generate_pairs(result)
```

    ## 4470×2 DataFrame
    ## │ Row  │ recordid │ recordid_1 │
    ## │      │ Int32    │ Int32      │
    ## ├──────┼──────────┼────────────┤
    ## │ 1    │ 5        │ 1340       │
    ## │ 2    │ 6        │ 1099       │
    ## │ 3    │ 7        │ 1069       │
    ## │ 4    │ 16       │ 1521       │
    ## │ 5    │ 18       │ 1134       │
    ## │ 6    │ 19       │ 1380       │
    ## │ 7    │ 20       │ 1243       │
    ## ⋮
    ## │ 4463 │ 1393     │ 1603       │
    ## │ 4464 │ 1410     │ 1465       │
    ## │ 4465 │ 1410     │ 1598       │
    ## │ 4466 │ 1425     │ 1589       │
    ## │ 4467 │ 1458     │ 1582       │
    ## │ 4468 │ 1461     │ 1463       │
    ## │ 4469 │ 1465     │ 1598       │
    ## │ 4470 │ 1567     │ 1610       │

The [GitHub repo](https://github.com/tarakc02/blocking/) contains
example problems of different sizes, along with solutions (and timings).

# Performance

In [Ball’s
blogpost](https://hrdag.org/tech-notes/adaptive-blocking-writeup-1.html),
he pre-evaluates every conjunction of length 1, 2, or 3, and then
chooses from among these in order to produce the final rule. This turns
out to be reasonably fast, but is limited since the solution can not
accomodate conjunctions of more than 3 conjuncts. The local search
method presented here attains solutions in a comparable amount of time.
However, it is able to avoid evaluating large swaths of the solution
space by only considering the small neighborhood surrounding each step
of the search path that it traverses. This savings allows the search to
go deeper, resulting in much longer conjunctions when necessary.

In order to improve the run time, we’d want to focus on the construction
of new conjunctions – that is the operation that dominates each step of
the algorithm. To find improvements, we can look at either reducing the
time taken to construct a conjunction (doing faster work), or reducing
the number of conjunctions we have to evaluate (doing less work).

## Doing less work

At each step of the algorithm, `add_greedy` evaluates all of the
neighbors of the current conjunction. For example, if there are \(k\)
available atomic rules and we start out with an empty conjunction, then
the first step of the algorithm will have to evaluate \(k\)
conjunctions, and then the next step will have to evaluate \(k-1\)
conjunctions, and so forth. Perhaps there is a way to proceed while
evaluating only a subset of neighbors. For example, we could propose one
neighbor at a time, and accept a proposal with a probability
proportional to its `value/cost`.

## Doing faster work

The time required to construct a conjunction is dominated by the time
required to calculate its `cost`, so that’s where we’d want to focus our
efforts. I implemented one small optimization – I
[memoized](https://en.wikipedia.org/wiki/Memoization) the `cost()`
function. But it’s probably possible to speed up the `cost()` function
itself. Right now it looks like this (see [the
repo](https://github.com/tarakc02/blocking) for the full details):

``` julia
function cost(records, selected)
    group_sizes = [Dict{UInt64, Float64}() for d in 1:Threads.nthreads()]
    if length(selected) == 0
        n_recs = size(records, 1)
        return n_recs * (n_recs - 1) / 2
    end
    selected = collect(selected)
    @inbounds Threads.@threads for row = 1:size(records, 1)
        key = hash(records[row, selected])
        update_dict(key, group_sizes[Threads.threadid()])
    end
    gs = merge(+, group_sizes...)
    sum(size * (size - 1) / 2 for (key, size) in gs)
end;
```

That function takes advantage of Julia’s multithreading capabilities and
is pretty fast, but perhaps it could be further optimized.
