---
output:
    html_document:
        toc: true
        highlight: pygments
    md_document:
        variant: gfm
title: Searching for blocking rules 
---
```{r setup, include = FALSE}
knitr::opts_knit$set(root.dir = normalizePath(".."))
```

# Introduction

## A too-big problem

In our work, we often rely on multiple overlapping data sources in order to make inferences in the face of the sorts of [incompleteness and selection bias](https://hrdag.org/coreconcepts/) that characterize [convenience samples](https://hrdag.org/2013/04/05/convenience-samples-what-they-are/). But that leaves us with the problem of [entity resolution](https://en.wikipedia.org/wiki/Record_linkage) -- of all the records we have, from all of the sources we've included, which ones correspond to the same distinct entity? Given a suitable similarity metric over the records, we can answer this question by clustering the records using the similarity metric (in which case, each cluster is an entity).

Before we've even defined or selected the appropriate metric, we know that it will be somewhat expensive to calculate. If we wanted to calculate the full similarity matrix for all records in a database of $n$ records, that leaves us having to do $n^2$ of these expensive calculations, which quickly becomes too many.

Notice, though, that whatever similarity metric we use, and whatever procedure we use to assign records to clusters, we know that records that are not very similar should not end up in the same cluster. So we can "round" the similarity down to 0 for sufficiently dissimilar records, and we are left with the much smaller job of filling out the cells of a sparse matrix, rather than the full $n^2$ cells of a dense matrix.

But that all presumes that we can, without actually calculating all of the similarities, identify the pairs of records that are sufficiently similar (so that we only calculate those similarities). That is the problem I'll address in this post -- how to efficiently identify a small subset of candidates among all $n^2$ possible pairs of records that still contains all (or most) of the pairs that do in fact refer to the same entity?

This post builds on [this 5-part series by Patrick Ball](https://hrdag.org/tech-notes/adaptive-blocking-writeup-1.html)

## Blocking

There are a number of functions over the records in our database that map similar (but not necessarily identical) records to the same output value. For instance, $name(record)$ takes in a record and returns the string contained in the name column of that record. $date(record)$ does the same for dates. $soundex(name(record))$ returns the [Soundex code](https://en.wikipedia.org/wiki/Soundex) associated with the name column of a record, while $year(date(record))$ returns the year component of the date. We'll call these functions *rules*, since they correspond to a rule for generating candidate pairs: cut the data into blocks of records that have the same return value for a given rule (such as all records with the name "Gabriela Doe"), and only consider pairs within these blocks. This technique is known as blocking. See [here](https://hrdag.org/tech-notes/adaptive-blocking-writeup-2.html) for more examples of blocking rules.

The sorts of rules described above, by themselves, will in general be insufficient: for example, $year(date(record))$ may still generate too many pairs to be practical, while $name(record)$ may miss pairs that are in fact similar (imagine a record with the name "Gabby Doe"). Instead we'll produce numerous such rules and combine them into more complex rules. We combine rules via the $Conjunction$ operator, so that for example $name \wedge date$ consists of all blocks of records that have the same name and the same date. We'll attempt to find several conjunctions, and output the union of the pairs generated by each one. We call the operator that takes two rules and returns a new rule that generates the union of the pairs from each constituent rule the $Disjunction$ operator. So our final rule could be like:

$$
(name \wedge date) \vee
(location \wedge soundex(name) \wedge year(date)) \vee
(...)
$$

Through large amounts of manual effort, we collect labeled training data that tells us, for a given pair of records, if those records represent the same underlying entity. Then our task is to find a disjunction of conjunctions, as above, that identifies as many of the true pairs in the training data as possible, subject to a constraint on how many total pairs we're willing to generate. Alas, if we have developed $k$ simple rules, there are $2^k$ distinct conjunctions of these rules, and $2^{2^k}$ disjunctions of conjunctions!

# Data

I simulated some training data that contains duplicates, and a number of columns that can be used as simple rules:

```{julia}
using Feather
records = Feather.read("input/small-recs.feather");
pairs = Feather.read("input/small-pairs.feather");
records
```

`pairs` contains the record ids of each identified pair, along with indicators for each blocking rule of whether that pair would be included in the blocks defined by the rule:

```{julia}
pairs
```

# Solution framework

I call the total number of pairs generated by a rule the `cost` of the rule, and the number of true matches included within that total the `value` of the rule. I seek a combination of rules that maximizes total `value`, subject to a fixed `budget`. The plan is to search for conjunctions bit by bit -- I search for a good conjunction, then remove the true pairs that are covered by that conjunction from the set of identified pairs in order to create a new problem, with the reduced set of identified pairs and the reduced budget that results from subtracting the cost of the conjunction from the original budget. In particular, I'll try to find conjunctions that have a high ratio of `value` to `cost` at each iteration, and continue until there are no more pairs to find, or I run out of `budget`.

I used [Julia](https://julialang.org/) to search for rules. The type `Blocking.Problem` contains the relevant information related to a problem:

```{julia}
Base.MainInclude.include("../block/src/block.jl");
using .Blocking
problem = Blocking.Problem(
    records,
    pairs,
    # array of candidate rules
    [n for n in propertynames(records) if n âˆ‰ (:id, :recordid)],
    # budget
    5000);
problem.npairs
```

In order to search for good conjunctions, I use a type of [neighborhood
search](https://en.wikipedia.org/wiki/Local_search_(optimization)). The idea is
to start with some proposed solution, and make small modifications until I
reach some stopping criterion. Each small modification is called a *local
move,* and the set of all available local moves from a given proposed solution
is called the *neighborhood* of that solution. In this case, the *neighborhood*
of a given conjunction consists of all conjunctions that I can create by adding
or removing exactly one conjunct from the conjunction.

So at the highest level, I'm looking to build an array of conjunctions, each
one aimed at covering any true pairs that were missed by the previous
conjunctions:

```{julia}
using .Blocking.Conjunctions
using .Blocking.Search

function search(problem)
    result = Blocking.Solution[]
    while problem.npairs > 0
        solution = Blocking.Search.greedy(
            problem,
            Blocking.Conjunctions.empty(problem),
            Blocking.Search.add_greedy)
        solution.cost > problem.budget && break
        solution.value == 0 && break
        append!(result, [solution])
        problem = Blocking.Conjunctions.subproblem(solution)
    end
    return result
end
```

Within each iteration, I use `Blocking.Search.greedy` to find the next
conjunction. For the conjunction search, I begin with an empty conjunction, and
add conjuncts one at a time. At each step, I add the conjunct that results in
the highest `value/cost` ratio, and stop when I can no longer improve the
ratio. This logic is implemented in the module `Blocking.Search`.
Specifically, `greedy` is a function that takes a blocking problem, a
starting point, and a function that advances from one conjunction to the next
until there are no remaining local moves that would lead to an improvement:

The code for `Blocking.search.greedy` is not too complicated -- 

```{julia, eval = FALSE}
function greedy(problem, start, advance)
    current = start
    next = advance(current)
    while next != current
        current = next
        next = advance(current)
    end
    return next
end;
```

The `advance` function should take a conjunction and return a new conjunction that is in the neighborhood of available local moves. It should return the original conjunction if there are no local moves available that are improvements on the current position. The one bit of complication to `Blocking.Search.add_greedy` is that I want to avoid returning a conjunction whose cost would exceed my budget. I do that by continuing to add conjuncts until I have an affordable conjunction:

```{julia, eval = FALSE}
function add_greedy(conjunction)
    problem = conjunction.problem
    current = conjunction
    current_ratio = current.value / current.cost
    for rule in conjunction.unselected
        candidate = Conjunctions.and(conjunction, rule)
        candidate_ratio = candidate.value / candidate.cost
        candidate_ratio < current_ratio && continue
        if candidate_ratio > current_ratio
            current = candidate
            current_ratio = candidate_ratio
            continue
        end

        if candidate.value > current.value || candidate.cost < current.cost
            current = candidate
            current_ratio = candidate_ratio
        end
    end
    # if we end up with a solution that is too expensive, just add more
    # conjuncts until we have something acceptable
    if current.cost > problem.budget && !isempty(current.unselected)
        current == conjunction && return current
        return add_greedy(current)
    end
    return current
end;
```

# Results

```{julia}
result = search(problem);
[show(rule) for rule in result]
# number of true matches identified:
sum([rule.value for rule in result])
# total number of pairs generated:
sum([rule.cost for rule in result])
```
The neighborhood search strategy results in conjunctions of various sizes, that together efficiently cover almost all of the true matches. In order to emit the pairs:

```{julia}
# emits pairs from a single conjunction
function emit(solution)
    recs = solution.problem.records
    cols = union(collect(solution.selected), [:recordid])
    blocks = filter(row -> row.recordid < row.recordid_1,
                    join(solution.problem.records[!, cols],
                         solution.problem.records[!, cols],
                         on = collect(solution.selected), makeunique = true))
    blocks[!, [:recordid, :recordid_1]]
end;

# takes the union of all pairs emitted by each conjunction
# [a; b] binds the rows of data frames a and b
function generate_pairs(result)
    reduce((a,b) -> [a;b], [emit(solution) for solution in result])
end;
generate_pairs(result)
```

The [GitHub repo](https://github.com/tarakc02/blocking/) contains example problems of different sizes, along with solutions (and timings).

# Performance

In [Ball's blogpost](https://hrdag.org/tech-notes/adaptive-blocking-writeup-1.html), he pre-evaluates every conjunction of length 1, 2, or 3, and then chooses from among these in order to produce the final rule. This turns out to be reasonably fast, but is limited since the solution can not accomodate conjunctions of more than 3 conjuncts. The local search method presented here attains solutions in a comparable amount of time. However, it is able to avoid evaluating large swaths of the solution space by only considering the small neighborhood surrounding each step of the search path that it traverses. This savings allows the search to go deeper, resulting in much longer conjunctions when necessary.

In order to improve the run time, we'd want to focus on the construction of new conjunctions -- that is the operation that dominates each step of the algorithm. To find improvements, we can look at either reducing the time taken to construct a conjunction (doing faster work), or reducing the number of conjunctions we have to evaluate (doing less work).

## Doing less work

At each step of the algorithm, `add_greedy` evaluates all of the neighbors of the current conjunction. For example, if there are $k$ available atomic rules and we start out with an empty conjunction, then the first step of the algorithm will have to evaluate $k$ conjunctions, and then the next step will have to evaluate $k-1$ conjunctions, and so forth. Perhaps there is a way to proceed while evaluating only a subset of neighbors. For example, we could propose one neighbor at a time, and accept a proposal with a probability proportional to its `value/cost`.

## Doing faster work

The time required to construct a conjunction is dominated by the time required to calculate its `cost`, so that's where we'd want to focus our efforts. I implemented one small optimization -- I [memoized](https://en.wikipedia.org/wiki/Memoization) the `cost()` function. But it's probably possible to speed up the `cost()` function itself. Right now it looks like this (see [the repo](https://github.com/tarakc02/blocking) for the full details):

```{julia}
function cost(records, selected)
    group_sizes = [Dict{UInt64, Float64}() for d in 1:Threads.nthreads()]
    if length(selected) == 0
        n_recs = size(records, 1)
        return n_recs * (n_recs - 1) / 2
    end
    selected = collect(selected)
    @inbounds Threads.@threads for row = 1:size(records, 1)
        key = hash(records[row, selected])
        update_dict(key, group_sizes[Threads.threadid()])
    end
    gs = merge(+, group_sizes...)
    sum(size * (size - 1) / 2 for (key, size) in gs)
end;
```

That function takes advantage of Julia's multithreading capabilities and is pretty fast, but perhaps it could be further optimized.
