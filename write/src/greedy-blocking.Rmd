---
output:
    html_document:
        keep_md: true
        toc: true
        df_print: paged
title: Searching for blocking rules 
---
```{r setup, include = FALSE}
knitr::opts_knit$set(root.dir = normalizePath(".."))
```

# Introduction

## A too-big problem

In our work, we often rely on multiple overlapping data sources in order to make inferences in the face of the sorts of [incompleteness and selection bias](https://hrdag.org/coreconcepts/) that characterize [convenience samples](https://hrdag.org/2013/04/05/convenience-samples-what-they-are/). But that leaves us with the problem of [entity resolution](https://en.wikipedia.org/wiki/Record_linkage) -- of all the records we have, from all of the sources we've included, which ones correspond to the same distinct entity? Given a suitable similarity metric over the records, we can answer this question by clustering the records using the similarity metric (in which case, each cluster is an entity).

Before we've even defined or selected the appropriate metric, we know that it will be somewhat expensive to calculate. If we wanted to calculate the full similarity matrix for all records in a database of $n$ records, that leaves us having to do $n^2$ of these expensive calculations, which quickly becomes too many.

Notice, though, that whatever similarity metric we use, and whatever procedure we use to assign records to clusters, we know that records that are not very similar should not end up in the same cluster. So we can "round" the similarity down to 0 for sufficiently dissimilar records, and we are left with the much smaller job of filling out the cells of a sparse matrix, rather than the full $n^2$ cells of a dense matrix.

But that all presumes that we can, without actually calculating all of the similarities, identify the pairs of records that are sufficiently similar (so that we only calculate those similarities). That is the problem I'll address in this post -- how to efficiently identify a small subset of candidates among all $n^2$ possible pairs of records that still contains all (or most) of the pairs that do in fact refer to the same entity?

This post builds on [this 5-part series by Patrick Ball](https://hrdag.org/tech-notes/adaptive-blocking-writeup-1.html)

## Blocking

There are a number of functions over the records in our database that map similar (but not necessarily identical) records to the same output value. For instance, $name(record)$ takes in a record and returns the string contained in the name column of that record. $date(record)$ does the same for dates. $soundex(name(record))$ returns the [Soundex code](https://en.wikipedia.org/wiki/Soundex) associated with the name column of a record, while $year(date(record))$ returns the year component of the date. We'll call these functions *rules*, since they correspond to a rule for generating candidate pairs: cut the data into blocks of records that have the same return value for a given rule (such as all records with the name "Gabriela Doe"), and only consider pairs within these blocks. This technique is known as blocking. See [here](https://hrdag.org/tech-notes/adaptive-blocking-writeup-2.html) for more examples of blocking rules.

The sorts of rules described above, by themselves, will in general be insufficient: for example, $year(date(record))$ may still generate too many pairs to be practical, while $name(record)$ may miss pairs that are in fact similar (imagine a record with the name "Gabby Doe"). Instead we'll produce numerous such rules and combine them into more complex rules. We combine rules via the $Conjunction$ operator, so that for example $name \wedge date$ consists of all blocks of records that have the same name and the same date. We'll attempt to find several conjunctions, and output the union of the pairs generated by each one. We call the operator that takes two rules and returns a new rule that generates the union of the pairs from each constituent rule the $Disjunction$ operator. So our final rule could be like:

$$
(name \wedge date) \vee
(location \wedge soundex(name) \wedge year(date)) \vee
(...)
$$

Through large amounts of manual effort, we collect labeled training data that tells us, for a given pair of records, if those records represent the same underlying entity. Then our task is to find a disjunction of conjunctions, as above, that identifies as many of the true pairs in the training data as possible, subject to a constraint on how many total pairs we're willing to generate. Alas, if we have developed $k$ simple rules, there are $2^k$ distinct conjunctions of these rules, and $2^{2^k}$ disjunctions of conjunctions!

# Data

I simulated some training data that contains duplicates, and a number of columns that can be used as simple rules:

```{r}
library(feather)
records <- read_feather("input/small-recs.feather")
pairs <- read_feather("input/small-pairs.feather")
records
```

`pairs` contains the record ids of each identified pair, along with indicators for each blocking rule of whether that pair would be included in the blocks defined by the rule:

```{r}
pairs
```

# Solution framework

I call the total number of pairs generated by a rule as the `cost` of the rule, and the number of true matches included within that total as the `value` of the rule. I seek a combination of rules that maximizes total `value`, subject to a fixed `budget`. The plan is to search for conjunctions bit by bit -- I search for a good conjunction, then remove the true pairs that are covered by that conjunction from the set of identified pairs in order to create a new problem, with the reduced set of identified pairs and the reduced budget that results from subtracting the cost of the conjunction from the original budget. In particalar, I'll try to find conjunctions that have a high ratio of `value` to `cost` at each iteration, and continue until there are no more pairs to find, or I run out of `budget`.

I used [Julia](https://julialang.org/) to search for rules. The type `Blocking.Problem` contains the relevant information related to a problem:

```{julia}
include("../block/block.jl")
using .Blocking
problem = Blocking.problem(
    records = records,
    matches = pairs,
    candidates = [n for n in names(records) if n âˆ‰ (:id, :recordid)],
    budget = 5000);
problem.npairs
```

In order to search for good conjunctions, I use a type of [neighborhood
search](https://en.wikipedia.org/wiki/Local_search_(optimization)). The idea is
to start with some proposed solution, and make small modifications until I
reach some stopping criterion. Each small modification is called a "local
move," and the set of all available local moves from a given proposed solution is called the "neighborhood" of that solution. In this case, the "neighborhood" of a given conjunction consists of all conjunctions that I can create by adding or removing exactly one conjunct from the conjunction.

So at the highest level, I'm looking to build an array of conjunctions, each one aimed at covering any true pairs that were missed by the previous conjunctions:

```{julia}
using .Blocking.Conjunctions
using .Blocking.Search

function search(problem)
    result = Blocking.Solution[]
    while problem.npairs > 0
        solution = Blocking.Search.greedy(
            problem = problem,
            start = Blocking.Conjunctions.empty(problem),
            advance = Blocking.Search.add_greedy)
        solution.cost > problem.budget && break
        solution.value == 0 && break
        append!(result, [solution])
        problem = Blocking.Conjunctions.subproblem(solution)
    end
    return result
end
```

Within each iteration, I use `Blocking.Search.greedy` to find the next
conjunction. For the conjunction search, I begin with an empty conjunction, and
add conjuncts one at a time. At each step, I add the conjunct that results in
the highest `value/cost` ratio, and stop when I can no longer improve the
ratio. This logic is implemented in the module `Blocking.Search`.
Specifically, `greedy` is a function that takes a blocking problem, a
starting point, and a function that advances from one conjunction to the next
until there are no remaining local moves that would lead to an improvement:

The code for `Blocking.search.greedy` is not too complicated -- 

```{julia}
function search_greedy(problem, start, advance)
    current = start
    next = advance(current)
    while next != current
        current = next
        next = advance(current)
    end
    return next
end
```

The `advance` function should take a conjunction and return a conjunction that is in the neighborhood of available local moves. It should return the original conjunction if there are no local moves available that are improvements on the current position. `Blocking.Search.add_greedy` is a little more complicated, since I want to avoid returning a conjunction whose cost would exceed my budget. I do that by continuing to add conjuncts until I have an affordable conjunction:

```{julia}
function add_greedy(conjunction)
    problem = conjunction.problem
    current = conjunction
    current_ratio = current.value / current.cost
    for rule in conjunction.unselected
        candidate = Conjunctions.and(conjunction, rule)

        candidate_ratio = candidate.value / candidate.cost
        candidate_ratio < current_ratio && continue
        if candidate_ratio > current_ratio
            current = candidate
            current_ratio = candidate_ratio
            continue
        end

        if candidate.value > current.value || candidate.cost < current.cost
            current = candidate
            current_ratio = candidate_ratio
        end
    end
    # if we end up with a solution that is too expensive, just add more
    # conjuncts until we have something acceptable
    if current.cost > problem.budget && !isempty(current.unselected)
        current == conjunction && return current
        return add_greedy(current)
    end
    return current
end
```

