---
output:
    html_document:
        keep_md: true
        toc: true
        df_print: paged
title: Searching for blocking rules 
---
```{r setup, include = FALSE}
knitr::opts_knit$set(root.dir = normalizePath(".."))
```

# Introduction

## A too-big problem

In our work, we often rely on multiple overlapping data sources in order to make inferences in the face of the sorts of [incompleteness and selection bias](https://hrdag.org/coreconcepts/) that characterize [convenience samples](https://hrdag.org/2013/04/05/convenience-samples-what-they-are/). But that leaves us with the problem of [entity resolution](https://en.wikipedia.org/wiki/Record_linkage) -- of all the records we have, from all of the sources we've included, which ones correspond to the same distinct entity? Given a suitable similarity metric over the records, we can answer this question by clustering the records using the similarity metric (in which case, each cluster is an entity).

Before we've even defined or selected the appropriate metric, we know that it will be somewhat expensive to calculate. If we wanted to calculate the full similarity matrix for all records in a database of $n$ records, that leaves us having to do $n^2$ of these expensive calculations, which quickly becomes too many.

Notice, though, that whatever similarity metric we use, and whatever procedure we use to assign records to clusters, we know that records that are not very similar should not end up in the same cluster. So we can "round" the similarity down to 0 for sufficiently dissimilar records, and we are left with the much smaller job of filling out the cells of a sparse matrix, rather than the full $n^2$ cells of a dense matrix.

But that all presumes that we can, without actually calculating all of the similarities, identify the pairs of records that are sufficiently similar (so that we only calculate those similarities). That is the problem I'll address in this post -- how to efficiently identify a small subset of candidates among all $n^2$ possible pairs of records that still contains all (or most) of the pairs that do in fact refer to the same entity?

This post builds on [this 5-part series by Patrick Ball](https://hrdag.org/tech-notes/adaptive-blocking-writeup-1.html)

## Blocking

There are a number of functions over the records in our database that map similar (but not necessarily identical) records to the same output value. For instance, $name(record)$ takes in a record and returns the string contained in the name column of that record. $date(record)$ does the same for dates. $soundex(name(record))$ returns the [Soundex code](https://en.wikipedia.org/wiki/Soundex) associated with the name column of a record, while $year(date(record))$ returns the year component of the date. We'll call these functions *rules*, since they correspond to a rule for generating candidate pairs: cut the data into blocks of records that have the same return value for a given rule (such as all records with the name "Gabriela Doe"), and only consider pairs within these blocks. This technique is known as blocking. See [here](https://hrdag.org/tech-notes/adaptive-blocking-writeup-2.html) for more examples of blocking rules.

The sorts of rules described above, by themselves, will in general be insufficient: for example, $year(date(record))$ may still generate too many pairs to be practical, while $name(record)$ may miss pairs that are in fact similar (imagine a record with the name "Gabby Doe"). Instead we'll produce numerous such rules and combine them into more complex rules. We combine rules via the $Conjunction$ operator, so that for example $name \wedge date$ consists of all blocks of records that have the same name and the same date. We'll attempt to find several conjunctions, and output the union of the pairs generated by each one. We call the operator that takes two rules and returns a new rule that generates the union of the pairs from each constituent rule the $Disjunction$ operator. So our final rule could be like:

$$
(name \wedge date) \vee
(location \wedge soundex(name) \wedge year(date)) \vee
(...)
$$

Through large amounts of manual effort, we collect labeled training data that tells us, for a given pair of records, if those records represent the same underlying entity. Then our task is to find a disjunction of conjunctions, as above, that identifies as many of the true pairs in the training data as possible, subject to a constraint on how many total pairs we're willing to generate. Alas, if we have developed $k$ simple rules, there are $2^k$ distinct conjunctions of these rules, and $2^{2^k}$ disjunctions of conjunctions!

# Data

I simulated some training data that contains duplicates, and a number of columns that can be used as simple rules:

```{r}
library(feather)
records <- read_feather("input/small-recs.feather")
pairs <- read_feather("input/small-pairs.feather")
records
```

`pairs` contains the record ids of each identified pair, along with indicators for each blocking rule of whether that pair would be included in the blocks defined by the rule:

```{r}
pairs
```

# Solution framework

I used [Julia](https://julialang.org/) to search for rules.
